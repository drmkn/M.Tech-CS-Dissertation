# Dissertation: Causal Explanations in Deep Learning Systems

This repository contains the code, data and experiments for the M.Tech Computer Science dissertation at the Indian Statistical Institute, Kolkata, titled **"Causal Explanations in Deep Learning Systems"**.

---

## ğŸ“– Thesis Overview

In this work, we propose and implement **Intrinsic Causal Contribution (ICC)**, a novel method to quantify the causal influence of input features on a neural networkâ€™s prediction, beyond correlations induced by other features. We:

- Model neural networks as Structural Causal Models (SCMs).
- Use **Causal Normalizing Flows (CNFs)** to learn the causal generative process of input features.
- Estimate ICC via the **Jansen estimator** for variance-based sensitivity analysis.
- Compare ICC against popular global attribution methods (SHAP, LIME, GAM, PFI, etc.) on synthetic and real-world datasets.


## ğŸ“° Published Paper

**S. Saha, D. V. Rathore, S. Saha, U. Garain, D. Doermann**, *"On Measuring Intrinsic Causal Attributions in Deep Neural Networks"* Accepted at Causal Learning and Reasoning (CLeaR) 2025. [arXiv:2505.09660](https://arxiv.org/abs/2505.09660)

## ğŸ“¦ Repository Structure

```
Dissertation/
â”œâ”€â”€ .vscode/                          # Editor settings
â”œâ”€â”€ assets/                           # Figures and diagrams
â”œâ”€â”€ attributions/                     # Saved attribution maps
â”œâ”€â”€ dags_estimated/                   # Estimated DAG structures
â”œâ”€â”€ datasets/                         # Synthetic and benchmark data
â”œâ”€â”€ explanations/                     # Generated explanations and examples
â”œâ”€â”€ models/                           # Trained model checkpoints
â”œâ”€â”€ .gitignore                        # Files and directories to ignore in Git
â”œâ”€â”€ ICC.py                            # ICC estimator implementation
â”œâ”€â”€ architectures.py                  # Neural network architectures 
â”œâ”€â”€ attributions.ipynb                # Interactive notebook for attribution methods
â”œâ”€â”€ dataloader.py                     # Data loading utilities
â”œâ”€â”€ estimate_DAG.py                   # DAG estimation scripts
â”œâ”€â”€ generate_attributions.py          # Pipeline to compute global attributions
â”œâ”€â”€ kde_visualisation_callback.py     # KDE visualization helper
â”œâ”€â”€ pl_modules.py                     # PyTorch Lightning modules
â”œâ”€â”€ trainer.py                        # Training and evaluation script
â””â”€â”€ utils.py                          # Utility functions
```

## ğŸ”§ Key Dependencies

The main libraries used in this project are:

- `torch`: the core deep learning framework for building and training neural networks.
- `pytorch-lightning`: a high-level wrapper for organizing training loops, logging and checkpointing.
- `zuko`: provides Causal Normalizing Flow implementations (e.g., Masked Autoregressive Flows) for modeling SCMs.
- `gam`: Generalized Additive Models for summarizing local feature attributions.
- `openxai`: toolkit for generating a variety of attribution methods (Integrated Gradients, Input Ã— Gradient, SmoothGrad, SHAP).
- `captum`: PyTorch interpretability library used here for permutation feature importance (PFI) and Lime/Splime baselines.

## âš™ï¸ Required Packages

Install the core libraries via pip:

```bash
pip install torch zuko openxai captum gam pytorch-lightning
```

## ğŸ™ Acknowledgements

I would like to express my sincere gratitude to Prof. [Utpal Garain](https://scholar.google.co.in/citations?user=4Jlqf30AAAAJ\&hl=en) for his invaluable guidance and support at the Indian Statistical Institute, Kolkata. I am also deeply thankful to [Saptarshi Saha](https://github.com/Saptarshi-Saha-1996) for his help and significant contributions to this work.





